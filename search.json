[
  {
    "objectID": "example_2.html",
    "href": "example_2.html",
    "title": "Example #2",
    "section": "",
    "text": "Compare models with fresh data"
  },
  {
    "objectID": "example_2.html#introduction",
    "href": "example_2.html#introduction",
    "title": "Example #2",
    "section": "",
    "text": "Compare models with fresh data"
  },
  {
    "objectID": "example_2.html#example-2",
    "href": "example_2.html#example-2",
    "title": "Example #2",
    "section": "Example #2",
    "text": "Example #2\n\nrequire(flexplot)\n\nLoading required package: flexplot\n\nrequire(lme4)\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:flexplot':\n\n    flip_data\n\n\n\nFirst look at the data\n\ndata(math)\nhead(math,10)\n\n   School Minority    Sex    SES MathAch MEANSES\n1    1224       No Female -1.528   5.876  -0.428\n2    1224       No Female -0.588  19.708  -0.428\n3    1224       No   Male -0.528  20.349  -0.428\n4    1224       No   Male -0.668   8.781  -0.428\n5    1224       No   Male -0.158  17.898  -0.428\n6    1224       No   Male  0.022   4.583  -0.428\n7    1224       No Female -0.618  -2.832  -0.428\n8    1224       No   Male -0.998   0.523  -0.428\n9    1224       No Female -0.888   1.527  -0.428\n10   1224       No   Male -0.458  21.521  -0.428\n\nglimpse(math)\n\nRows: 7,185\nColumns: 6\n$ School   &lt;int&gt; 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1…\n$ Minority &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Sex      &lt;fct&gt; Female, Female, Male, Male, Male, Male, Female, Male, Female,…\n$ SES      &lt;dbl&gt; -1.528, -0.588, -0.528, -0.668, -0.158, 0.022, -0.618, -0.998…\n$ MathAch  &lt;dbl&gt; 5.876, 19.708, 20.349, 8.781, 17.898, 4.583, -2.832, 0.523, 1…\n$ MEANSES  &lt;dbl&gt; -0.428, -0.428, -0.428, -0.428, -0.428, -0.428, -0.428, -0.42…\n\nmath %&gt;% group_by(Minority, Sex) %&gt;% summarise(.groups = \"keep\")\n\n# A tibble: 4 × 2\n# Groups:   Minority, Sex [4]\n  Minority Sex   \n  &lt;fct&gt;    &lt;fct&gt; \n1 No       Female\n2 No       Male  \n3 Yes      Female\n4 Yes      Male  \n\nmath %&gt;% group_by(School) %&gt;% summarise(.groups = \"keep\")\n\n# A tibble: 160 × 1\n# Groups:   School [160]\n   School\n    &lt;int&gt;\n 1   1224\n 2   1288\n 3   1296\n 4   1308\n 5   1317\n 6   1358\n 7   1374\n 8   1433\n 9   1436\n10   1461\n# ℹ 150 more rows\n\nsummary(math)\n\n     School     Minority       Sex            SES               MathAch      \n Min.   :1224   No :5211   Female:3795   Min.   :-3.758000   Min.   :-2.832  \n 1st Qu.:3020   Yes:1974   Male  :3390   1st Qu.:-0.538000   1st Qu.: 7.275  \n Median :5192                            Median : 0.002000   Median :13.131  \n Mean   :5278                            Mean   : 0.000143   Mean   :12.748  \n 3rd Qu.:7342                            3rd Qu.: 0.602000   3rd Qu.:18.317  \n Max.   :9586                            Max.   : 2.692000   Max.   :24.993  \n    MEANSES         \n Min.   :-1.188000  \n 1st Qu.:-0.317000  \n Median : 0.038000  \n Mean   : 0.006138  \n 3rd Qu.: 0.333000  \n Max.   : 0.831000  \n\n\n\nComments\n\nFrom the above, we have 160 schools with two genders classified as minority or not.\nSES and MEANSES look like distributions centered around 0\nMathAch is centered around ~13 and has -ve scores! Curious how one can attain a negative achievement score in Math: must be somehow a normalized variable.\nDo some plots to look more closely at distributions\n\n\n# plots \nggplot(math, aes(MathAch)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\nggplot(math, aes(SES)) +\n  geom_histogram(binwidth = 0.05)\n\n\n\nggplot(math, aes(MEANSES)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\nVariable Meanings\n\nSES is Socio-Economic Status or Score?\nMEANSES is presumable the mean of SES that is already calculated for each school ID\nMathAch is achievement in Math?\n\n\n\nNo predictor model\n\n# no predictor\nmod_baseline &lt;- lmer(MathAch~1 + (1|School), data=math)\nvisualize(mod_baseline, plot = \"model\", sample = 12)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\nsummary(mod_baseline)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ 1 + (1 | School)\n   Data: math\n\nREML criterion at convergence: 47116.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0631 -0.7539  0.0267  0.7606  2.7426 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n School   (Intercept)  8.614   2.935   \n Residual             39.148   6.257   \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.6370     0.2444   51.71\n\n\n\n\nFirst model with predictors: fixed and random effects\n\n# SES as fixed and random effect\nmod_ses &lt;- lmer(MathAch~SES + (SES|School), data=math)\nvisualize(mod_ses, plot = \"model\", sample = 6)\n\n\n\nsummary(mod_ses)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ SES + (SES | School)\n   Data: math\n\nREML criterion at convergence: 46640.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.12272 -0.73046  0.02144  0.75610  2.94356 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n School   (Intercept)  4.8287  2.1974        \n          SES          0.4129  0.6426   -0.11\n Residual             36.8301  6.0688        \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.6650     0.1898   66.71\nSES           2.3938     0.1181   20.27\n\nCorrelation of Fixed Effects:\n    (Intr)\nSES -0.045\n\n\n\nComments\nFixed effect is very clear with individual schools being close to parallel to fixed effect\n\n\n\nCompare baseline and first model\n\n# compare models\nmodel.comparison(mod_baseline, mod_ses)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                  aic      bic bayes.factor      p\nmod_baseline 47122.79 47143.43 0.000000e+00 &lt;2e-16\nmod_ses      46652.40 46693.68 4.605423e+97       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.514 1.071 1.802 8.991 \n\n$r_squared_change\n   Residual (Intercept) \n 0.05921519  0.43943350 \n\n\n\nComments\n\nBayes Factor favors mod_ses and great p value\nR^2 change (by using mod_ses that has SES predictor):\n\nexplains 5.92 % of the residual variance. Difference between fitted models for each cluster (in this case, each school).\n43.94 % of variance in intercept (relative to the mod_base that has no predictor - juts the mean of every data point)\n\nVisualize that….\n\n\n# see the difference with 3 example schools\ncompare.fits(MathAch~SES | School, mod_baseline, mod_ses, data = math)\n\n\n\n\n\n\nComments\n\nPlots are narrow\nsample=n is not observed\n\n\n\n\nA better model? Let’s see\n\n# a better model including Minority?\nmod_ses_minority &lt;- lmer(MathAch~SES + Minority + (SES|School), data=math)\nvisualize(mod_ses_minority, plot = \"model\", sample = 6)\n\n\n\nsummary(mod_ses_minority)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ SES + Minority + (SES | School)\n   Data: math\n\nREML criterion at convergence: 46443\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1742 -0.7243  0.0283  0.7577  3.0047 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n School   (Intercept)  3.9362  1.9840        \n          SES          0.3187  0.5645   -0.40\n Residual             35.9967  5.9997        \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  13.5047     0.1827   73.92\nSES           2.1326     0.1156   18.44\nMinorityYes  -2.9782     0.2081  -14.31\n\nCorrelation of Fixed Effects:\n            (Intr) SES   \nSES         -0.194       \nMinorityYes -0.307  0.173\n\n\n\nCompare …\n\n\n# better model?\n# compare models\nmodel.comparison(mod_baseline, mod_ses_minority)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                      aic      bic  bayes.factor      p\nmod_baseline     47122.79 47143.43  0.000000e+00 &lt;2e-16\nmod_ses_minority 46457.03 46505.19 3.917437e+138       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.564 1.187 2.028 7.690 \n\n$r_squared_change\n   Residual (Intercept) \n 0.08050517  0.54305152 \n\n\n\nBayes Factor is much better: 3.917437e+138 vs 4.605423e+97\nR^2 change (by using mod_ses_minority that has SES and Minority predictors):\n\nexplains 8.05 % of the residual variance. Difference between fitted models for each cluster (in this case, each school) vs ~6%\n54.31 % of variance in intercept (relative to the mod_base that has no predictor - just the mean of every data point)\n\nVisualize that….\n\n\n# see the difference with 3 example schools\ncompare.fits(MathAch~SES | Minority + School, mod_baseline, mod_ses_minority, data = math)"
  },
  {
    "objectID": "example_2.html#what-about-a-regular-linear-models",
    "href": "example_2.html#what-about-a-regular-linear-models",
    "title": "Example #2",
    "section": "What about a regular linear models",
    "text": "What about a regular linear models\n\nsome additional ggplots will help with understanding variables\n\n\n# it's a factor\nmath$School &lt;- as.factor(math$School)\n\n\n1 Predictor\n\n# SES with Minority colouring\nggplot(math, aes(x=SES, y=MathAch, colour=Minority)) + \n  geom_point(size=0.1) + \n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = TRUE, color=\"blue\")\n\n\n\n# model 1 factor \nlm1 &lt;- lm(MathAch ~ SES, data = math)\nsummary(lm1)\n\n\nCall:\nlm(formula = MathAch ~ SES, data = math)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.4382  -4.7580   0.2334   5.0649  15.9007 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.74740    0.07569  168.42   &lt;2e-16 ***\nSES          3.18387    0.09712   32.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.416 on 7183 degrees of freedom\nMultiple R-squared:  0.1301,    Adjusted R-squared:   0.13 \nF-statistic:  1075 on 1 and 7183 DF,  p-value: &lt; 2.2e-16\n\nvisualize(lm1, plot=\"model\")\n\n\n\nvisualize(lm1, plot=\"residuals\")\n\n\n\n\n\nComments\n\nIt’s not obvious from the scatter-plot points that SES will be a good predictor - though regression line with SE shows the correlation.\nMinorityYes points can be seen emerging in bottom left - less SES ~ less MathAch\nSES is highly significant though the Adjusted R-squared is small (0.13)\n\n\n\n\n2 Predictors\n\n# Belt and braces: Minority box and violin\nggplot(math, aes(x=Minority, y=MathAch, colour=Minority)) + \n  geom_violin() + \n  geom_jitter(shape=16, position=position_jitter(0.4), size=0.3) +\n  geom_boxplot(width=0.1, colour=\"black\") \n\n\n\n# 2 factors\nlm2 &lt;- lm(MathAch ~ SES + Minority, data = math)\nsummary(lm2)\n\n\nCall:\nlm(formula = MathAch ~ SES + Minority, data = math)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.6824  -4.6338   0.2277   4.9502  17.1271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.52473    0.08822  153.31   &lt;2e-16 ***\nSES          2.74398    0.09909   27.69   &lt;2e-16 ***\nMinorityYes -2.82912    0.17299  -16.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.3 on 7182 degrees of freedom\nMultiple R-squared:  0.1614,    Adjusted R-squared:  0.1611 \nF-statistic:   691 on 2 and 7182 DF,  p-value: &lt; 2.2e-16\n\nvisualize(lm2, plot=\"model\")\n\n\n\nvisualize(lm2, plot=\"residuals\")\n\n\n\n\n\nComments\n\nBox Violin plot show clearly the effect of Minority\nMedian is about 5 points down for Yes\nSES and Minority together are better predictors\nHow to visualize a model using 2 or more indepandant variables?\n\nlook at this nice example plucked from the interweb\n\n\n\n# read dataset\ndf = mtcars\n\n# create multiple linear model\nlm_fit &lt;- lm(mpg ~ cyl + hp, data=df)\n#summary(lm_fit)\n\n# save predictions of the model in the new data frame \n# together with variable you want to plot against\npredicted_df &lt;- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)\n\n# this is the predicted line of multiple linear regression\nggplot(data = df, aes(x = mpg, y = hp)) + \n  geom_point(color='blue') +\n  geom_line(color='red',data = predicted_df, aes(x=mpg_pred, y=hp))\n\n\n\n\n\nAnd now the same approach for the 2 factor model, lm2:\n\nlm2 &lt;- lm(MathAch ~ SES + Minority, data = math)\n\n\n\n# save predictions of the model in the new data frame \n# together with variable you want to plot against\npredicted_df &lt;- data.frame(MathArc_pred = predict(lm2, math), SES=math$SES)\n\n# this is the predicted line of multiple linear regression\nggplot(data = math, aes(x = SES, y = MathAch)) + \n  geom_point(color='blue',size=0.1) +\n  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))\n\n\n\n\n\n\n\nDown the rabbit hole\n\n# interesting \n# what does this mean.. 3 predictors...shows\nlm3 &lt;- lm(MathAch ~ SES + Minority + School, data = math)\n# summary(lm3)\n# undo above comment to go down the hole\n#\n# Does this mean that the model with SES + Minority is better for some schools?\n# TO DO\n\n\n\nCompare models\n\n# compare standard 1 and 2 predictor models\nmodel.comparison(lm1, lm2)\n\n$statistics\n         aic      bic bayes.factor      p   rsq\nlm1 47103.94 47124.58 0.000000e+00 &lt;2e-16 0.130\nlm2 46843.23 46870.75 1.312994e+55        0.161\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.051 0.565 0.886 1.472 3.236 \n\n\n\nComments\n\nlm2 better than 1\n\n\n# compare the standard 2 predictor model with the MM with 2 predictors \nmodel.comparison(lm2, mod_ses_minority)\n\n$statistics\n                      aic      bic bayes.factor\nlm2              46843.23 46870.75 0.000000e+00\nmod_ses_minority 46457.03 46505.19 2.402547e+79\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.495 1.097 1.965 7.403 \n\n\n\n\nComments\n\nMM better than standard 2 predictor variable model?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Reference examples from the excellent teachings of Dustin Fife and other sources to inspire my own work and teachings (About me)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Mixed-Effect Models",
    "section": "",
    "text": "Some reference examples of Linear Mixed-Effect Models (LMMs or just MMs for short) otherwise known as Hierarchical Linear Models (HLMs) or Multi Level Models."
  },
  {
    "objectID": "example_1.html",
    "href": "example_1.html",
    "title": "Example #1",
    "section": "",
    "text": "Some examples of using “flexplot” and “lmer” based on explanations from Dustin Fife (Video Link). Modified with tidyverse packages and plots (ggplot) to enable additional data understanding. My annotations and observations in line with code. Wrapped up in a Quarto package."
  },
  {
    "objectID": "example_1.html#introduction",
    "href": "example_1.html#introduction",
    "title": "Example #1",
    "section": "",
    "text": "Some examples of using “flexplot” and “lmer” based on explanations from Dustin Fife (Video Link). Modified with tidyverse packages and plots (ggplot) to enable additional data understanding. My annotations and observations in line with code. Wrapped up in a Quarto package."
  },
  {
    "objectID": "example_1.html#example-1",
    "href": "example_1.html#example-1",
    "title": "Example #1",
    "section": "Example #1",
    "text": "Example #1\n\nrequire(flexplot)\n\nLoading required package: flexplot\n\nrequire(lme4)\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nData about alcohol use in teenagers is distributed with flexplot.\n\ndata(alcuse)\n# see some data\nhead(alcuse, 10)\n\n   ID AGE COA MALE AGE_14   ALCUSE      PEER      CPEER  CCOA\n1   1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2   1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3   1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4   2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5   2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6   2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n7   3  14   1    1      0 1.000000 0.8944272 -0.1235728 0.549\n8   3  15   1    1      1 2.000000 0.8944272 -0.1235728 0.549\n9   3  16   1    1      2 3.316625 0.8944272 -0.1235728 0.549\n10  4  14   1    1      0 0.000000 1.7888544  0.7708544 0.549\n\n# rows\npaste0(nrow(alcuse), \" rows of data.\")\n\n[1] \"246 rows of data.\"\n\n# number of groups and data points\nalcuse_sum &lt;- alcuse %&gt;% group_by(ID) %&gt;% \n  summarise(mean = mean(ALCUSE), n_ALCUSE = n()) %&gt;% print(n = 20)\n\n# A tibble: 82 × 3\n      ID  mean n_ALCUSE\n   &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;\n 1     1 1.91         3\n 2     2 0.333        3\n 3     3 2.11         3\n 4     4 1.24         3\n 5     5 0            3\n 6     6 3.05         3\n 7     7 1.73         3\n 8     8 0            3\n 9     9 1.49         3\n10    10 1            3\n11    11 1.05         3\n12    12 0.911        3\n13    13 0.667        3\n14    14 3.09         3\n15    15 2.14         3\n16    16 1.28         3\n17    17 0.333        3\n18    18 1.58         3\n19    19 0            3\n20    20 2.64         3\n# ℹ 62 more rows\n\n\n\n# number of groups check \n# 3 measures for each ID\nnrow(filter(alcuse_sum, n_ALCUSE == 3))\n\n[1] 82\n\n\n\nComments\nThere are 3 measurements of ALCUSE for each of 82 IDs.\nProbably a boxplot is more useful to get a feel for the data (TODO)\n\nsummary(alcuse)\n\n       ID            AGE          COA              MALE            AGE_14 \n Min.   : 1.0   Min.   :14   Min.   :0.0000   Min.   :0.0000   Min.   :0  \n 1st Qu.:21.0   1st Qu.:14   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0  \n Median :41.5   Median :15   Median :0.0000   Median :1.0000   Median :1  \n Mean   :41.5   Mean   :15   Mean   :0.4512   Mean   :0.5122   Mean   :1  \n 3rd Qu.:62.0   3rd Qu.:16   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2  \n Max.   :82.0   Max.   :16   Max.   :1.0000   Max.   :1.0000   Max.   :2  \n     ALCUSE           PEER            CPEER                 CCOA           \n Min.   :0.000   Min.   :0.0000   Min.   :-1.0180000   Min.   :-0.4510000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:-1.0180000   1st Qu.:-0.4510000  \n Median :1.000   Median :0.8944   Median :-0.1235728   Median :-0.4510000  \n Mean   :0.922   Mean   :1.0176   Mean   :-0.0004405   Mean   : 0.0002195  \n 3rd Qu.:1.732   3rd Qu.:1.5492   3rd Qu.: 0.5311934   3rd Qu.: 0.5490000  \n Max.   :3.606   Max.   :2.5298   Max.   : 1.5118221   Max.   : 0.5490000  \n\n\n\n\n\nModel of alcohol use. No predictors.\n\n# Fixed effect + random effects\n# Fixed effect: ~1 is gammma 00\n# Random effects: 1 is Uij for ID \nmod = lmer(ALCUSE~1 + (1|ID), data=alcuse)\nvisualize(mod, plot = \"model\", sample = 20)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\nsummary(mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + (1 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 673\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.5731   0.7571  \n Residual             0.5617   0.7495  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9220     0.0963   9.574\n\n\n\nComments\nDustin’s model visualize() plot does not look like this one despite the code being identical.\nThe visualize function defaults to 3 random IDs (?). I change it (with sample = n)\nLooks like Dustin’s data is normalized to 1\nDustin gets Fixed Effect and Random Effects plotted. I just get: object (lmerMod).\nHowever the summary(mod) is identical to Dustin’s.\n\n# always useful to look this way\nsummary(mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + (1 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 673\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.5731   0.7571  \n Residual             0.5617   0.7495  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9220     0.0963   9.574\n\n\n\n\nComments\nThe model Intercept 0.9220 is simply the mean of the ALCUSE variable\n\nsignif(mean(alcuse$ALCUSE),4)\n\n[1] 0.922\n\n\n\n\n\nRandom intercepts model\n\n# random intercepts using AGE_14 as predictor for fixed IDs\n# AGE_14 predictor (fixed gradients)\n# (1|ID) tells intercepts may vary (random intercepts)\nrand.i &lt;- lmer(ALCUSE~1 + AGE_14 + (1|ID), data = alcuse)\n# by default 3 IDs are plotted\nsample_n &lt;- 6\n# if all groups are plotted, one gets funny comments :)\n# very nice humour\n# sample_n &lt;- 82\nvisualize(rand.i, plot = \"model\", sample=sample_n)\n\n\n\nsummary(rand.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + AGE_14 + (1 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 654.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.19816 -0.66940  0.03001  0.44728  2.66167 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.5966   0.7724  \n Residual             0.4915   0.7011  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.11077   5.880\nAGE_14       0.27065    0.05474   4.944\n\nCorrelation of Fixed Effects:\n       (Intr)\nAGE_14 -0.494\n\n\n\nComments\nFixed effect (Intercept) means 0.65130 drinks on average at age 14.\nFor subsequent years the number of drinks increases on average by 0.27065.\n\n\n\nRandom slopes (gradient) model\n\n# unrealistic \n# -1 means intercept is fixed\n# need to consider the level 1 and 2 models to specify the syntax correctly\nrand.s &lt;- lmer(ALCUSE~1 + AGE_14 + (-1 + AGE_14|ID), data = alcuse)\nsample_n &lt;- 10\nvisualize(rand.s, plot = \"model\", sample=sample_n)\n\nIt looks like you're trying to plot more than 6 colors/lines/symbols.\nI gotta give it to you...you're ambitious. Alas, I can't do that, so I'm removing the colors/lines/symbols.\n I hope we can still be friends.\n\n\n\n\nsummary(rand.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + AGE_14 + (-1 + AGE_14 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 697.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5231 -0.7636 -0.4583  0.4239  3.1493 \n\nRandom effects:\n Groups   Name   Variance Std.Dev.\n ID       AGE_14 0.2225   0.4717  \n Residual        0.7163   0.8463  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.08532   7.634\nAGE_14       0.27065    0.08415   3.216\n\nCorrelation of Fixed Effects:\n       (Intr)\nAGE_14 -0.608\n\n\n\nComments\nDespite the warning, visualize seems to do the job with 10 samples\n\n\n\nRandom slopes and intercepts model\n\n# probably what we want\n# need to consider the level 1 and 2 models to specify the syntax correctly\nrand.is &lt;- lmer(ALCUSE~1 + AGE_14 + (AGE_14|ID), data = alcuse)\nsample_n &lt;- 10\nvisualize(rand.is, plot = \"model\", sample=sample_n)\n\nWarning in compare.fits(formula, data = data, model1 = object, re = T, return.preds = T, : Some of the model's predicted values are beyond the range of the original y-values. \n              I'm truncating the y-axis to preserve the original scale.\n\n\nIt looks like you're trying to plot more than 6 colors/lines/symbols.\nI gotta give it to you...you're ambitious. Alas, I can't do that, so I'm removing the colors/lines/symbols.\n I hope we can still be friends.\n\n\n\n\nsummary(rand.is)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + AGE_14 + (AGE_14 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 643.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.48287 -0.37933 -0.07858  0.38876  2.49284 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept) 0.6355   0.7972        \n          AGE_14      0.1552   0.3939   -0.23\n Residual             0.3373   0.5808        \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.10573   6.160\nAGE_14       0.27065    0.06284   4.307\n\nCorrelation of Fixed Effects:\n       (Intr)\nAGE_14 -0.441\n\n\n\nComments\n\nok"
  }
]