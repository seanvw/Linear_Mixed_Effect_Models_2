[
  {
    "objectID": "example_3.html",
    "href": "example_3.html",
    "title": "Example #3",
    "section": "",
    "text": "Look into Intra Class Correlation (ICC)\nWhat proportion of variability (variance) in the model is due to cluster effects"
  },
  {
    "objectID": "example_3.html#introduction",
    "href": "example_3.html#introduction",
    "title": "Example #3",
    "section": "",
    "text": "Look into Intra Class Correlation (ICC)\nWhat proportion of variability (variance) in the model is due to cluster effects"
  },
  {
    "objectID": "example_3.html#example-3",
    "href": "example_3.html#example-3",
    "title": "Example #3",
    "section": "Example #3",
    "text": "Example #3\n\nrequire(flexplot)\n\nLoading required package: flexplot\n\nrequire(lme4)\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#require(ggplot2)"
  },
  {
    "objectID": "example_3.html#fit-a-baseline-and-compute-icc",
    "href": "example_3.html#fit-a-baseline-and-compute-icc",
    "title": "Example #3",
    "section": "Fit a baseline and compute ICC",
    "text": "Fit a baseline and compute ICC\n\ndata(math)\nbaseline &lt;- lmer(MathAch~1 + (1|School), data=math) \n# ICC is var(school)/ ( var(school) + var(person) ) \n# e.g. when school and person vars are equal ICC is 0.5\n# Increasing person var relative to school pushes toward 1 \n# ICC runs 0 to 1\n# ICC = 0 means perfectly independant \n# rising ICC towards 1 (or 100%) is towards non-independant\n# This function extracts the necessary variances from the mixed model and computes ICC.\n# Digging in the icc() function one can see it uses lme4 function VarrCorr to get variances.\n#\n# VarCorr {lme4} Extract Variance and Correlation Components\n# This function calculates the estimated variances, standard deviations, and correlations \n# between the random-effects terms in a mixed-effects model, of class merMod (linear, \n# generalized or nonlinear). The within-group error variance and standard deviation are also # calculated.\n#\n# Digging: In this case try, \n# str(as.data.frame(VarCorr(baseline))\n# grp  : chr  \"School\" \"Residual\"\n# ..\n# vcov : num  8.61 39.15\n# \n# ICC is ~ 8.61 / (8.61 + 39.15) = 0.1802764\n# vcov is \"variances or covariances\"\n# Using MathAch var by school from model fit\n# calculating directly, gives  different numbers\n# grp_school &lt;- math %&gt;% group_by(School) %&gt;% summarise(mean_MathAch = mean(MathAch)) \n# var(grp_school$mean_MathAch) \n# 9.71975\n# var(math$MathAch)\n# 47.31026\n# \n# so, perhaps ICC in this case is better expressed as..understood by me as...\n# from the model fit\n# var(MathAch by school)/ ( var(MathAch by school) + var(MathAch disregarding School) ) \n\nicc(baseline) \n\n$icc\n[1] 0.1803518\n\n$design.effect\n[1] 8.918571\n\nvisualize(baseline, plot=\"model\")\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\nComments\n\n18% cluster effects\nmy plot doesn’t show the random effects despite the model being identical to DFs\n\n\nfixed_slopes &lt;- lmer(MathAch~SES + (1|School), data=math)\nvisualize(fixed_slopes, plot=\"model\")\n\n\n\n\n\nrandom_slopes &lt;- lmer(MathAch~SES + (SES|School), data=math)\nvisualize(random_slopes, plot=\"model\")\n\n\n\n\n\n# see 8 schools, 2 at a time \ncompare.fits(MathAch~SES | School, data=math, fixed_slopes, random_slopes, clusters = 2)\n\n\n\ncompare.fits(MathAch~SES | School, data=math, fixed_slopes, random_slopes, clusters = 2)\n\n\n\ncompare.fits(MathAch~SES | School, data=math, fixed_slopes, random_slopes, clusters = 2)\n\n\n\ncompare.fits(MathAch~SES | School, data=math, fixed_slopes, random_slopes, clusters = 2)\n\n\n\n\n\n# \nmodel.comparison(fixed_slopes, baseline)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                  aic      bic  bayes.factor      p\nfixed_slopes 46653.17 46680.69 3.045609e+100 &lt;2e-16\nbaseline     47122.79 47143.43  0.000000e+00       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.532 1.095 1.825 8.917 \n\n$r_squared_change\n   Residual (Intercept) \n  0.0539978   0.4464638 \n\nmodel.comparison(random_slopes, baseline)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                   aic      bic bayes.factor      p\nrandom_slopes 46652.40 46693.68 4.605423e+97 &lt;2e-16\nbaseline      47122.79 47143.43 0.000000e+00       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.514 1.071 1.802 8.991 \n\n$r_squared_change\n   Residual (Intercept) \n 0.05921519  0.43943350 \n\nmodel.comparison(fixed_slopes, random_slopes)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                   aic      bic bayes.factor     p\nfixed_slopes  46653.17 46680.69      661.309 0.104\nrandom_slopes 46652.40 46693.68        0.002      \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.024 0.070 0.162 2.008 \n\n$r_squared_change\n    Residual  (Intercept) \n-0.005545779  0.012541409 \n\n# fixed_slopes is probably better\nsummary(fixed_slopes)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ SES + (1 | School)\n   Data: math\n\nREML criterion at convergence: 46645.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.12607 -0.72720  0.02188  0.75772  2.91912 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n School   (Intercept)  4.768   2.184   \n Residual             37.034   6.086   \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.6575     0.1880   67.33\nSES           2.3902     0.1057   22.61\n\nCorrelation of Fixed Effects:\n    (Intr)\nSES 0.003 \n\n\n\n# genrally categorical vars are fixed effects\nwith_minority =  lmer(MathAch~SES + Minority + (SES|School), data=math)\ncompare.fits(MathAch~SES | Minority + School, data=math, with_minority, fixed_slopes)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\nmodel.comparison(with_minority, fixed_slopes)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                   aic      bic bayes.factor      p\nwith_minority 46457.03 46505.19 1.286257e+38 &lt;2e-16\nfixed_slopes  46653.17 46680.69 0.000000e+00       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.197 0.461 0.990 3.603 \n\n$r_squared_change\n   Residual (Intercept) \n 0.02802041  0.17449215 \n\nsummary(with_minority)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ SES + Minority + (SES | School)\n   Data: math\n\nREML criterion at convergence: 46443\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1742 -0.7243  0.0283  0.7577  3.0047 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n School   (Intercept)  3.9362  1.9840        \n          SES          0.3187  0.5645   -0.40\n Residual             35.9967  5.9997        \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  13.5047     0.1827   73.92\nSES           2.1326     0.1156   18.44\nMinorityYes  -2.9782     0.2081  -14.31\n\nCorrelation of Fixed Effects:\n            (Intr) SES   \nSES         -0.194       \nMinorityYes -0.307  0.173\n\n\n\n\nComments\n\nAkaike information criterion (AIC)\nBayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC)\nLooking at summary(with_minority), the model parameters are pretty similar to Example #2 - the standard linear model (lm2 &lt;- lm(MathAch ~ SES + Minority, data = math))\n\n\n# software library update\n# estimates() has become available\n\n# explain of dessign.effect - artificail infaltion \nestimates(with_minority)\n\nrefitting model(s) with ML (instead of REML)\n\n\nFixed Effects: \n(Intercept)         SES MinorityYes \n  13.504724    2.132604   -2.978157 \n\n\nRandom Effects: \n Groups   Name        Std.Dev. Corr  \n School   (Intercept) 1.9840         \n          SES         0.5645   -0.400\n Residual             5.9997         \n\n\nICC and Design Effect: \n          icc design.effect \n    0.1803518     8.9185709 \n\n\nR Squared: \n\n   Residual (Intercept) \n 0.08050517  0.54305152"
  },
  {
    "objectID": "example_1.html",
    "href": "example_1.html",
    "title": "Example #1",
    "section": "",
    "text": "Some examples of using “flexplot” and “lmer” based on the excellent explanations from Dustin Fife (Video Link). Modified with tidyverse packages and plots (ggplot) to enable additional data understanding. My annotations and observations in line with code. Wrapped up in a Quarto package."
  },
  {
    "objectID": "example_1.html#introduction",
    "href": "example_1.html#introduction",
    "title": "Example #1",
    "section": "",
    "text": "Some examples of using “flexplot” and “lmer” based on the excellent explanations from Dustin Fife (Video Link). Modified with tidyverse packages and plots (ggplot) to enable additional data understanding. My annotations and observations in line with code. Wrapped up in a Quarto package."
  },
  {
    "objectID": "example_1.html#example-1",
    "href": "example_1.html#example-1",
    "title": "Example #1",
    "section": "Example #1",
    "text": "Example #1\n\nrequire(flexplot)\n\nLoading required package: flexplot\n\nrequire(lme4)\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nData about alcohol use in teenagers is distributed with flexplot.\n\ndata(alcuse)\n# see some data\nhead(alcuse, 10)\n\n   ID AGE COA MALE AGE_14   ALCUSE      PEER      CPEER  CCOA\n1   1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2   1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3   1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4   2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5   2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6   2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n7   3  14   1    1      0 1.000000 0.8944272 -0.1235728 0.549\n8   3  15   1    1      1 2.000000 0.8944272 -0.1235728 0.549\n9   3  16   1    1      2 3.316625 0.8944272 -0.1235728 0.549\n10  4  14   1    1      0 0.000000 1.7888544  0.7708544 0.549\n\n# rows\npaste0(nrow(alcuse), \" rows of data.\")\n\n[1] \"246 rows of data.\"\n\n# number of groups and data points\nalcuse_sum &lt;- alcuse %&gt;% group_by(ID) %&gt;% \n  summarise(mean = mean(ALCUSE), n_ALCUSE = n()) %&gt;% print(n = 20)\n\n# A tibble: 82 × 3\n      ID  mean n_ALCUSE\n   &lt;int&gt; &lt;dbl&gt;    &lt;int&gt;\n 1     1 1.91         3\n 2     2 0.333        3\n 3     3 2.11         3\n 4     4 1.24         3\n 5     5 0            3\n 6     6 3.05         3\n 7     7 1.73         3\n 8     8 0            3\n 9     9 1.49         3\n10    10 1            3\n11    11 1.05         3\n12    12 0.911        3\n13    13 0.667        3\n14    14 3.09         3\n15    15 2.14         3\n16    16 1.28         3\n17    17 0.333        3\n18    18 1.58         3\n19    19 0            3\n20    20 2.64         3\n# ℹ 62 more rows\n\n\n\n# number of groups check \n# 3 measures for each ID\nnrow(filter(alcuse_sum, n_ALCUSE == 3))\n\n[1] 82\n\n\n\nComments\nThere are 3 measurements of ALCUSE for each of 82 IDs.\nProbably a boxplot is more useful to get a feel for the data (TODO)\n\nsummary(alcuse)\n\n       ID            AGE          COA              MALE            AGE_14 \n Min.   : 1.0   Min.   :14   Min.   :0.0000   Min.   :0.0000   Min.   :0  \n 1st Qu.:21.0   1st Qu.:14   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0  \n Median :41.5   Median :15   Median :0.0000   Median :1.0000   Median :1  \n Mean   :41.5   Mean   :15   Mean   :0.4512   Mean   :0.5122   Mean   :1  \n 3rd Qu.:62.0   3rd Qu.:16   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2  \n Max.   :82.0   Max.   :16   Max.   :1.0000   Max.   :1.0000   Max.   :2  \n     ALCUSE           PEER            CPEER                 CCOA           \n Min.   :0.000   Min.   :0.0000   Min.   :-1.0180000   Min.   :-0.4510000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:-1.0180000   1st Qu.:-0.4510000  \n Median :1.000   Median :0.8944   Median :-0.1235728   Median :-0.4510000  \n Mean   :0.922   Mean   :1.0176   Mean   :-0.0004405   Mean   : 0.0002195  \n 3rd Qu.:1.732   3rd Qu.:1.5492   3rd Qu.: 0.5311934   3rd Qu.: 0.5490000  \n Max.   :3.606   Max.   :2.5298   Max.   : 1.5118221   Max.   : 0.5490000  \n\n\n\n\n\nModel of alcohol use. No predictors.\n\n# Fixed effect + random effects\n# Fixed effect: ~1 is gammma 00\n# Random effects: 1 is Uij for ID \nmod = lmer(ALCUSE~1 + (1|ID), data=alcuse)\nvisualize(mod, plot = \"model\", sample = 20)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\nsummary(mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + (1 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 673\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.5731   0.7571  \n Residual             0.5617   0.7495  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9220     0.0963   9.574\n\n\n\nComments\nDustin’s model visualize() plot does not look like this one despite the code being identical.\nThe visualize function defaults to 3 random IDs (?). I change it (with sample = n)\nLooks like Dustin’s data is normalized to 1\nDustin gets Fixed Effect and Random Effects plotted. I just get: object (lmerMod).\nHowever the summary(mod) is identical to Dustin’s.\n\n# always useful to look this way\nsummary(mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + (1 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 673\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.5731   0.7571  \n Residual             0.5617   0.7495  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9220     0.0963   9.574\n\n\n\n\nComments\nThe model Intercept 0.9220 is simply the mean of the ALCUSE variable\n\nsignif(mean(alcuse$ALCUSE),4)\n\n[1] 0.922\n\n\n\n\n\nRandom intercepts model\n\n# random intercepts using AGE_14 as predictor for fixed IDs\n# AGE_14 predictor (fixed gradients)\n# (1|ID) tells intercepts may vary (random intercepts)\nrand.i &lt;- lmer(ALCUSE~1 + AGE_14 + (1|ID), data = alcuse)\n# by default 3 IDs are plotted\nsample_n &lt;- 6\n# if all groups are plotted, one gets funny comments :)\n# very nice humour\n# sample_n &lt;- 82\nvisualize(rand.i, plot = \"model\", sample=sample_n)\n\n\n\nsummary(rand.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + AGE_14 + (1 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 654.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.19816 -0.66940  0.03001  0.44728  2.66167 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.5966   0.7724  \n Residual             0.4915   0.7011  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.11077   5.880\nAGE_14       0.27065    0.05474   4.944\n\nCorrelation of Fixed Effects:\n       (Intr)\nAGE_14 -0.494\n\n\n\nComments\nFixed effect (Intercept) means 0.65130 drinks on average at age 14.\nFor subsequent years the number of drinks increases on average by 0.27065.\n\n\n\nRandom slopes (gradient) model\n\n# unrealistic \n# -1 means intercept is fixed\n# need to consider the level 1 and 2 models to specify the syntax correctly\nrand.s &lt;- lmer(ALCUSE~1 + AGE_14 + (-1 + AGE_14|ID), data = alcuse)\nsample_n &lt;- 10\nvisualize(rand.s, plot = \"model\", sample=sample_n)\n\nIt looks like you're trying to plot more than 6 colors/lines/symbols.\nI gotta give it to you...you're ambitious. Alas, I can't do that, so I'm removing the colors/lines/symbols.\n I hope we can still be friends.\n\n\n\n\nsummary(rand.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + AGE_14 + (-1 + AGE_14 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 697.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5231 -0.7636 -0.4583  0.4239  3.1493 \n\nRandom effects:\n Groups   Name   Variance Std.Dev.\n ID       AGE_14 0.2225   0.4717  \n Residual        0.7163   0.8463  \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.08532   7.634\nAGE_14       0.27065    0.08415   3.216\n\nCorrelation of Fixed Effects:\n       (Intr)\nAGE_14 -0.608\n\n\n\nComments\nDespite the warning, visualize seems to do the job with 10 samples\n\n\n\nRandom slopes and intercepts model\n\n# probably what we want\n# need to consider the level 1 and 2 models to specify the syntax correctly\nrand.is &lt;- lmer(ALCUSE~1 + AGE_14 + (AGE_14|ID), data = alcuse)\nsample_n &lt;- 10\nvisualize(rand.is, plot = \"model\", sample=sample_n)\n\nIt looks like you're trying to plot more than 6 colors/lines/symbols.\nI gotta give it to you...you're ambitious. Alas, I can't do that, so I'm removing the colors/lines/symbols.\n I hope we can still be friends.\n\n\n\n\nsummary(rand.is)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: ALCUSE ~ 1 + AGE_14 + (AGE_14 | ID)\n   Data: alcuse\n\nREML criterion at convergence: 643.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.48287 -0.37933 -0.07858  0.38876  2.49284 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept) 0.6355   0.7972        \n          AGE_14      0.1552   0.3939   -0.23\n Residual             0.3373   0.5808        \nNumber of obs: 246, groups:  ID, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.10573   6.160\nAGE_14       0.27065    0.06284   4.307\n\nCorrelation of Fixed Effects:\n       (Intr)\nAGE_14 -0.441\n\n\n\nComments\n\nok"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Mixed-Effect Models",
    "section": "",
    "text": "Always wanted to find a chuck of time to refine my statistical modelling knowledge beyond General Linear Models and Generalized Linear Models. Now realized with RStudio, Quarto and GitHub so that notes, code and examples are online for future reference (for a quicker push to brain RAM).\nThis website contains reference examples of Linear Mixed-Effect Models (LMMs or just MMs for short) otherwise known as Hierarchical Linear Models (HLMs) or Multi Level Models. Used when the assumption of independence is broken and General Linear Models are no longer appropriate (they do a pretty reasonable job though as a little aside in Example #2 shows). Independence is broken when same samples are measured multiple times (time course e.g. when alcohol use is measured each year within a fixed group of subjects) or when samples are correlated (clusters e.g. family members are likely not independent with respect to religious faith)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About me"
  },
  {
    "objectID": "example_2.html",
    "href": "example_2.html",
    "title": "Example #2",
    "section": "",
    "text": "Compare models with the data set ‘math’. Apart from the regular flow from DF, sections have been added on data set understanding together with a delve into regular GLM (General not Generalized!) and some funky visualizations of multi-factorial models."
  },
  {
    "objectID": "example_2.html#introduction",
    "href": "example_2.html#introduction",
    "title": "Example #2",
    "section": "",
    "text": "Compare models with the data set ‘math’. Apart from the regular flow from DF, sections have been added on data set understanding together with a delve into regular GLM (General not Generalized!) and some funky visualizations of multi-factorial models."
  },
  {
    "objectID": "example_2.html#example-2",
    "href": "example_2.html#example-2",
    "title": "Example #2",
    "section": "Example #2",
    "text": "Example #2\n\nrequire(flexplot)\n\nLoading required package: flexplot\n\nrequire(lme4)\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:flexplot':\n\n    flip_data\n\n\n\nFirst look at the data\n\ndata(math)\nhead(math,10)\n\n   School Minority    Sex    SES MathAch MEANSES\n1    1224       No Female -1.528   5.876  -0.428\n2    1224       No Female -0.588  19.708  -0.428\n3    1224       No   Male -0.528  20.349  -0.428\n4    1224       No   Male -0.668   8.781  -0.428\n5    1224       No   Male -0.158  17.898  -0.428\n6    1224       No   Male  0.022   4.583  -0.428\n7    1224       No Female -0.618  -2.832  -0.428\n8    1224       No   Male -0.998   0.523  -0.428\n9    1224       No Female -0.888   1.527  -0.428\n10   1224       No   Male -0.458  21.521  -0.428\n\nglimpse(math)\n\nRows: 7,185\nColumns: 6\n$ School   &lt;int&gt; 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1224, 1…\n$ Minority &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Sex      &lt;fct&gt; Female, Female, Male, Male, Male, Male, Female, Male, Female,…\n$ SES      &lt;dbl&gt; -1.528, -0.588, -0.528, -0.668, -0.158, 0.022, -0.618, -0.998…\n$ MathAch  &lt;dbl&gt; 5.876, 19.708, 20.349, 8.781, 17.898, 4.583, -2.832, 0.523, 1…\n$ MEANSES  &lt;dbl&gt; -0.428, -0.428, -0.428, -0.428, -0.428, -0.428, -0.428, -0.42…\n\nmath %&gt;% group_by(Minority, Sex) %&gt;% summarise(.groups = \"keep\")\n\n# A tibble: 4 × 2\n# Groups:   Minority, Sex [4]\n  Minority Sex   \n  &lt;fct&gt;    &lt;fct&gt; \n1 No       Female\n2 No       Male  \n3 Yes      Female\n4 Yes      Male  \n\nmath %&gt;% group_by(School) %&gt;% summarise(.groups = \"keep\")\n\n# A tibble: 160 × 1\n# Groups:   School [160]\n   School\n    &lt;int&gt;\n 1   1224\n 2   1288\n 3   1296\n 4   1308\n 5   1317\n 6   1358\n 7   1374\n 8   1433\n 9   1436\n10   1461\n# ℹ 150 more rows\n\nsummary(math)\n\n     School     Minority       Sex            SES               MathAch      \n Min.   :1224   No :5211   Female:3795   Min.   :-3.758000   Min.   :-2.832  \n 1st Qu.:3020   Yes:1974   Male  :3390   1st Qu.:-0.538000   1st Qu.: 7.275  \n Median :5192                            Median : 0.002000   Median :13.131  \n Mean   :5278                            Mean   : 0.000143   Mean   :12.748  \n 3rd Qu.:7342                            3rd Qu.: 0.602000   3rd Qu.:18.317  \n Max.   :9586                            Max.   : 2.692000   Max.   :24.993  \n    MEANSES         \n Min.   :-1.188000  \n 1st Qu.:-0.317000  \n Median : 0.038000  \n Mean   : 0.006138  \n 3rd Qu.: 0.333000  \n Max.   : 0.831000  \n\n\n\nComments\n\nFrom the above, we have 160 schools with two genders classified as minority or not.\nSES and MEANSES look like distributions centered around 0\nMathAch is centered around ~13 and has -ve scores! Curious how one can attain a negative achievement score in Math: must be somehow a normalized variable.\nDo some plots to look more closely at distributions\n\n\n# plots \nggplot(math, aes(MathAch)) +\n  geom_histogram(binwidth = 0.1)\n\n\n\nggplot(math, aes(SES)) +\n  geom_histogram(binwidth = 0.05)\n\n\n\nggplot(math, aes(MEANSES)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\nVariable Meanings\n\nSES is Socio-Economic Status or Score?\nMEANSES is presumable the mean of SES that is already calculated for each school ID\nMathAch is achievement in Math?\n\n\n\nNo predictor model\n\n# no predictor\nmod_baseline &lt;- lmer(MathAch~1 + (1|School), data=math)\nvisualize(mod_baseline, plot = \"model\", sample = 12)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\nsummary(mod_baseline)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ 1 + (1 | School)\n   Data: math\n\nREML criterion at convergence: 47116.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0631 -0.7539  0.0267  0.7606  2.7426 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n School   (Intercept)  8.614   2.935   \n Residual             39.148   6.257   \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.6370     0.2444   51.71\n\n\n\n\nFirst model with predictors: fixed and random effects\n\n# SES as fixed and random effect\nmod_ses &lt;- lmer(MathAch~SES + (SES|School), data=math)\nvisualize(mod_ses, plot = \"model\", sample = 6)\n\n\n\nsummary(mod_ses)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ SES + (SES | School)\n   Data: math\n\nREML criterion at convergence: 46640.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.12272 -0.73046  0.02144  0.75610  2.94356 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n School   (Intercept)  4.8287  2.1974        \n          SES          0.4129  0.6426   -0.11\n Residual             36.8301  6.0688        \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.6650     0.1898   66.71\nSES           2.3938     0.1181   20.27\n\nCorrelation of Fixed Effects:\n    (Intr)\nSES -0.045\n\n\n\nComments\nFixed effect is very clear with individual schools being close to parallel to fixed effect\n\n\n\nCompare baseline and first model\n\n# compare models\nmodel.comparison(mod_baseline, mod_ses)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                  aic      bic bayes.factor      p\nmod_baseline 47122.79 47143.43 0.000000e+00 &lt;2e-16\nmod_ses      46652.40 46693.68 4.605423e+97       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.514 1.071 1.802 8.991 \n\n$r_squared_change\n   Residual (Intercept) \n 0.05921519  0.43943350 \n\n\n\nComments\n\nBayes Factor favors mod_ses and great p value\nR^2 change (by using mod_ses that has SES predictor):\n\nexplains 5.92 % of the residual variance. Difference between fitted models for each cluster (in this case, each school).\n43.94 % of variance in intercept (relative to the mod_base that has no predictor - juts the mean of every data point)\n\nVisualize that….\n\n\n# see the difference with 3 example schools\ncompare.fits(MathAch~SES | School, mod_baseline, mod_ses, data = math)\n\n\n\n\n\n\nComments\n\nPlots are narrow\nsample=n is not observed\n\n\n\n\nA better model? Let’s see\n\n# a better model including Minority?\nmod_ses_minority &lt;- lmer(MathAch~SES + Minority + (SES|School), data=math)\nvisualize(mod_ses_minority, plot = \"model\", sample = 6)\n\n\n\nsummary(mod_ses_minority)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MathAch ~ SES + Minority + (SES | School)\n   Data: math\n\nREML criterion at convergence: 46443\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1742 -0.7243  0.0283  0.7577  3.0047 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n School   (Intercept)  3.9362  1.9840        \n          SES          0.3187  0.5645   -0.40\n Residual             35.9967  5.9997        \nNumber of obs: 7185, groups:  School, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  13.5047     0.1827   73.92\nSES           2.1326     0.1156   18.44\nMinorityYes  -2.9782     0.2081  -14.31\n\nCorrelation of Fixed Effects:\n            (Intr) SES   \nSES         -0.194       \nMinorityYes -0.307  0.173\n\n\n\nCompare … to understand “variance explained”\n\nWe get statistics on variance of\n\nResiduals (calculated over all model point deviations from individual group regression lines - random effects)\nIntercept (fixed effect versus random effects at the intercept)\nSlopes (comparison of individual group regressions - random effects vs group effects)\n\n\n\n\n# better model?\n# compare models\nmodel.comparison(mod_baseline, mod_ses_minority)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                      aic      bic  bayes.factor      p\nmod_baseline     47122.79 47143.43  0.000000e+00 &lt;2e-16\nmod_ses_minority 46457.03 46505.19 3.917437e+138       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.564 1.187 2.028 7.690 \n\n$r_squared_change\n   Residual (Intercept) \n 0.08050517  0.54305152 \n\n\n\nBayes Factor is much better: 3.917437e+138 vs 4.605423e+97\nR^2 change (by using mod_ses_minority that has SES and Minority predictors):\n\nexplains 8.05 % of the residual variance. Difference between fitted models for each cluster (in this case, each school) vs ~6%\n54.31 % of variance in intercept (relative to the mod_base that has no predictor - just the mean of every data point)\n\nVisualize that….\n\n\n# see the difference with 3 example schools\ncompare.fits(MathAch~SES | Minority + School, mod_baseline, mod_ses_minority, data = math)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\nCompare first and second models\n\n# compare models\nmodel.comparison(mod_ses, mod_ses_minority)\n\nrefitting model(s) with ML (instead of REML)\n\n\n$statistics\n                      aic      bic bayes.factor      p\nmod_ses          46652.40 46693.68 0.000000e+00 &lt;2e-16\nmod_ses_minority 46457.03 46505.19 8.506139e+40       \n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.206 0.428 0.957 3.183 \n\n$r_squared_change\n   Residual (Intercept)         SES \n 0.02263002  0.18484518  0.22830024 \n\n\n\n2.326 % improvement in residual fit\n18.48% improvement in intercept variance\nSES unexplained variance reduced by 22.83 %\n\n\n# see the difference with 3 example schools\ncompare.fits(MathAch~SES | Minority + School, mod_ses, mod_ses_minority, data = math)"
  },
  {
    "objectID": "example_2.html#what-about-a-regular-linear-models",
    "href": "example_2.html#what-about-a-regular-linear-models",
    "title": "Example #2",
    "section": "What about a regular linear models",
    "text": "What about a regular linear models\n\nsome additional ggplots will help with understanding variables\n\n\n# it's a factor\nmath$School &lt;- as.factor(math$School)\n\n\n1 Predictor\n\n# SES with Minority colouring\nggplot(math, aes(x=SES, y=MathAch, colour=Minority)) + \n  geom_point(size=0.1) + \n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = TRUE, color=\"blue\")\n\n\n\n# model 1 factor \nlm1 &lt;- lm(MathAch ~ SES, data = math)\nsummary(lm1)\n\n\nCall:\nlm(formula = MathAch ~ SES, data = math)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.4382  -4.7580   0.2334   5.0649  15.9007 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.74740    0.07569  168.42   &lt;2e-16 ***\nSES          3.18387    0.09712   32.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.416 on 7183 degrees of freedom\nMultiple R-squared:  0.1301,    Adjusted R-squared:   0.13 \nF-statistic:  1075 on 1 and 7183 DF,  p-value: &lt; 2.2e-16\n\nvisualize(lm1, plot=\"model\")\n\n\n\nvisualize(lm1, plot=\"residuals\")\n\n\n\n\n\nComments\n\nIt’s not obvious from the scatter-plot points that SES will be a good predictor - though regression line with SE shows the correlation.\nMinorityYes points can be seen emerging in bottom left - less SES ~ less MathAch\nSES is highly significant though the Adjusted R-squared is small (0.13)\n\n\n\n\n2 Predictors\n\n# Belt and braces: Minority box and violin\nggplot(math, aes(x=Minority, y=MathAch, colour=Minority)) + \n  geom_violin() + \n  geom_jitter(shape=16, position=position_jitter(0.4), size=0.3) +\n  geom_boxplot(width=0.1, colour=\"black\") \n\n\n\n# and let's have a look at Sex now we are here\nggplot(math, aes(x=Sex, y=MathAch, colour=Sex)) + \n  geom_violin() + \n  geom_jitter(shape=16, position=position_jitter(0.4), size=0.3) +\n  geom_boxplot(width=0.1, colour=\"black\") \n\n\n\n# 2 factors\nlm2 &lt;- lm(MathAch ~ SES + Minority, data = math)\nsummary(lm2)\n\n\nCall:\nlm(formula = MathAch ~ SES + Minority, data = math)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.6824  -4.6338   0.2277   4.9502  17.1271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.52473    0.08822  153.31   &lt;2e-16 ***\nSES          2.74398    0.09909   27.69   &lt;2e-16 ***\nMinorityYes -2.82912    0.17299  -16.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.3 on 7182 degrees of freedom\nMultiple R-squared:  0.1614,    Adjusted R-squared:  0.1611 \nF-statistic:   691 on 2 and 7182 DF,  p-value: &lt; 2.2e-16\n\nvisualize(lm2, plot=\"model\")\n\n\n\nvisualize(lm2, plot=\"residuals\")\n\n\n\n\n\nComments\n\nBox Violin plot show clearly the effect of Minority\nMedian is about 5 points down for Yes\nSES and Minority together are better predictors\nHow to visualize a model using 2 or more indepandant variables?\n\nlook at this nice example plucked from the interweb\n\n\n\n# read dataset\ndf = mtcars\n\n# create multiple linear model\nlm_fit &lt;- lm(mpg ~ cyl + hp, data=df)\n#summary(lm_fit)\n\n# save predictions of the model in the new data frame \n# together with variable you want to plot against\npredicted_df &lt;- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)\n\n# this is the predicted line of multiple linear regression\nggplot(data = df, aes(x = mpg, y = hp)) + \n  geom_point(color='blue') +\n  geom_line(color='red',data = predicted_df, aes(x=mpg_pred, y=hp))\n\n\n\n\n\nAnd now the same approach for the 2 factor model, lm2:\n\nlm2 &lt;- lm(MathAch ~ SES + Minority, data = math)\n\n\n\n# save predictions of the model in the new data frame \n# together with variable you want to plot against\npredicted_df &lt;- data.frame(MathArc_pred = predict(lm2, math), SES=math$SES)\n\n# this is the predicted line of multiple linear regression\nggplot(data = math, aes(x = SES, y = MathAch)) + \n  geom_point(color='blue',size=0.1) +\n  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))\n\n\n\n\n\n\n\n3 Predictors\n\n# interesting \n# what does this mean.. 3 predictors...shows\nlm3 &lt;- lm(MathAch ~ SES + Minority + School, data = math)\n# summary(lm3)\n# undo above comment to go down the hole\n#\n# Does this mean that the model with SES + Minority is better for some schools?\n# TO DO\n\n\n# 3 predictors\nlm3 &lt;- lm(MathAch ~ SES + Minority + MEANSES, data = math)\nsummary(lm3)\n\n\nCall:\nlm(formula = MathAch ~ SES + Minority + MEANSES, data = math)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.4561  -4.5692   0.1964   4.8371  17.6516 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.36822    0.08786  152.16   &lt;2e-16 ***\nSES          1.99956    0.11202   17.85   &lt;2e-16 ***\nMinorityYes -2.32435    0.17476  -13.30   &lt;2e-16 ***\nMEANSES      2.92260    0.21421   13.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.22 on 7181 degrees of freedom\nMultiple R-squared:  0.1826,    Adjusted R-squared:  0.1822 \nF-statistic: 534.6 on 3 and 7181 DF,  p-value: &lt; 2.2e-16\n\npredicted_df &lt;- data.frame(MathArc_pred = predict(lm3, math), SES=math$SES)\n# this is the predicted line of multiple linear regression\nggplot(data = math, aes(x = SES, y = MathAch)) + \n  geom_point(color='blue',size=0.1) +\n  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))\n\n\n\nggplot(data = math, aes(x = SES, y = MathAch)) + \n  geom_point(color='blue',size=0.1) +\n  geom_point(color='red',size=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))\n\n\n\n\n\n\n4 Predictors\n\n# 4 predictors\nlm4 &lt;- lm(MathAch ~ SES + Minority + MEANSES + Sex, data = math)\nsummary(lm4)\n\n\nCall:\nlm(formula = MathAch ~ SES + Minority + MEANSES + Sex, data = math)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.7498  -4.4732   0.2485   4.8209  18.0243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  12.7503     0.1111 114.772   &lt;2e-16 ***\nSES           1.9551     0.1115  17.533   &lt;2e-16 ***\nMinorityYes  -2.3410     0.1738 -13.469   &lt;2e-16 ***\nMEANSES       2.8675     0.2131  13.455   &lt;2e-16 ***\nSexMale       1.3200     0.1466   9.005   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.186 on 7180 degrees of freedom\nMultiple R-squared:  0.1917,    Adjusted R-squared:  0.1912 \nF-statistic: 425.7 on 4 and 7180 DF,  p-value: &lt; 2.2e-16\n\npredicted_df &lt;- data.frame(MathArc_pred = predict(lm4, math), SES=math$SES)\n# this is the predicted line of multiple linear regression\nggplot(data = math, aes(x = SES, y = MathAch)) + \n  geom_point(color='blue',size=0.1) +\n  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))\n\n\n\nggplot(data = math, aes(x = SES, y = MathAch)) + \n  geom_point(color='blue',size=0.1) +\n  geom_point(color='red',size=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))"
  },
  {
    "objectID": "example_2.html#compare-models",
    "href": "example_2.html#compare-models",
    "title": "Example #2",
    "section": "Compare models",
    "text": "Compare models\n\n# compare standard 1 and 2 predictor models\nmodel.comparison(lm1, lm2)\n\n$statistics\n         aic      bic bayes.factor      p   rsq\nlm1 47103.94 47124.58 0.000000e+00 &lt;2e-16 0.130\nlm2 46843.23 46870.75 1.312994e+55        0.161\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.051 0.565 0.886 1.472 3.236 \n\n\n\nComments\n\nlm2 better than lm1\n\n\n# compare 2 and 3 predictor models\nmodel.comparison(lm2, lm3)\n\n$statistics\n         aic      bic bayes.factor      p   rsq\nlm2 46843.23 46870.75 0.000000e+00 &lt;2e-16 0.161\nlm3 46661.35 46695.75 1.000671e+38        0.183\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.001 0.348 0.708 1.170 4.025 \n\n\n\n\nComments\n\nlm3 better than lm2\n\n\n# compare 3 and 4 predictor models\nmodel.comparison(lm3, lm4)\n\n$statistics\n         aic      bic bayes.factor      p   rsq\nlm3 46661.35 46695.75 0.000000e+00 &lt;2e-16 0.183\nlm4 46582.66 46623.93 3.934229e+15        0.192\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.477 0.612 0.655 0.697 0.875 \n\n\n\n\nComments\n\nlm4 better than lm3\n\n\n# compare 1 and 4 predictor models\nmodel.comparison(lm1, lm4)\n\n$statistics\n         aic      bic  bayes.factor      p   rsq\nlm1 47103.94 47124.58  0.000000e+00 &lt;2e-16 0.130\nlm4 46582.66 46623.93 5.169085e+108        0.192\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.542 1.159 1.964 6.671 \n\n\n\n\nComments\n\nbayes.factor seems to be additive lm1 &lt;- lm2 &lt;- lm3 &lt;-lm4\nlm4 is much better than lm1\n\n\n# compare the standard 2 predictor model with the MM with 2 predictors \nmodel.comparison(lm2, mod_ses_minority)\n\n$statistics\n                      aic      bic bayes.factor\nlm2              46843.23 46870.75 0.000000e+00\nmod_ses_minority 46457.03 46505.19 2.402547e+79\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.495 1.097 1.965 7.403 \n\n\n\n\nComments\n\nMM better than standard 2 predictor variable model?\n\n\n\n\n# compare the standard 2 predictor model with the MM with 2 predictors \nmodel.comparison(lm4, mod_ses_minority)\n\n$statistics\n                      aic      bic bayes.factor\nlm4              46582.66 46623.93 0.000000e+00\nmod_ses_minority 46457.03 46505.19 6.102683e+25\n\n$predicted_differences\n   0%   25%   50%   75%  100% \n0.000 0.511 1.045 1.826 7.019 \n\n\n\n\nComments\n\n.."
  }
]