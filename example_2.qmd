---
title: "Example #2"
---

## Introduction

Compare models with the data set 'math'. Apart from the regular flow from DF, sections have been added on data set understanding together with a delve into regular GLM (General not Generalized!) and some funky visualizations of multi-factorial models.

## Example #2

```{r}
require(flexplot)
require(lme4)
require(dplyr)
require(ggplot2)
```

### First look at the data

```{r}
data(math)
head(math,10)
glimpse(math)
math %>% group_by(Minority, Sex) %>% summarise(.groups = "keep")
math %>% group_by(School) %>% summarise(.groups = "keep")
summary(math)

```

#### Comments

-   From the above, we have 160 schools with two genders classified as minority or not.

-   SES and MEANSES look like distributions centered around 0

-   MathAch is centered around \~13 and has -ve scores! Curious how one can attain a negative achievement score in Math: must be somehow a normalized variable.

-   Do some plots to look more closely at distributions

```{r}
# plots 
ggplot(math, aes(MathAch)) +
  geom_histogram(binwidth = 0.1)

ggplot(math, aes(SES)) +
  geom_histogram(binwidth = 0.05)

ggplot(math, aes(MEANSES)) +
  geom_histogram(binwidth = 0.01)
```

### Variable Meanings

-   SES is Socio-Economic Status or Score?

-   MEANSES is presumable the mean of SES that is already calculated for each school ID

-   MathAch is achievement in Math?

### No predictor model

```{r}
# no predictor
mod_baseline <- lmer(MathAch~1 + (1|School), data=math)
visualize(mod_baseline, plot = "model", sample = 12)
summary(mod_baseline)
```

### First model with predictors: fixed and random effects

```{r}
# SES as fixed and random effect
mod_ses <- lmer(MathAch~SES + (SES|School), data=math)
visualize(mod_ses, plot = "model", sample = 6)
summary(mod_ses)
```

#### Comments

Fixed effect is very clear with individual schools being close to parallel to fixed effect

### Compare baseline and first model

```{r}
# compare models
model.comparison(mod_baseline, mod_ses)
```

#### Comments

-   Bayes Factor favors mod_ses and great p value

-   R\^2 change (by using mod_ses that has SES predictor):

    -   explains 5.92 % of the residual variance. Difference between fitted models for each cluster (in this case, each school).

    -   43.94 % of variance in intercept (relative to the mod_base that has no predictor - juts the mean of every data point)

-   Visualize that....

```{r}
# see the difference with 3 example schools
compare.fits(MathAch~SES | School, mod_baseline, mod_ses, data = math)
```

#### Comments

-   Plots are narrow

-   sample=n is not observed

### A better model? Let's see

```{r}
# a better model including Minority?
mod_ses_minority <- lmer(MathAch~SES + Minority + (SES|School), data=math)
visualize(mod_ses_minority, plot = "model", sample = 6)
summary(mod_ses_minority)

```

-   Compare ... to understand "variance explained"
    -   We get statistics on variance of
        -   Residuals (calculated over all model point deviations from individual group regression lines - random effects)
        -   Intercept (fixed effect versus random effects at the intercept)
        -   Slopes (comparison of individual group regressions - random effects vs group effects)

```{r}
# better model?
# compare models
model.comparison(mod_baseline, mod_ses_minority)
```

-   Bayes Factor is much better: 3.917437e+138 vs 4.605423e+97

-   R\^2 change (by using mod_ses_minority that has SES and Minority predictors):

    -   explains 8.05 % of the residual variance. Difference between fitted models for each cluster (in this case, each school) vs \~6%

    -   54.31 % of variance in intercept (relative to the mod_base that has no predictor - just the mean of every data point)

-   Visualize that....

```{r}
# see the difference with 3 example schools
compare.fits(MathAch~SES | Minority + School, mod_baseline, mod_ses_minority, data = math)
```

### Compare first and second models

```{r}
# compare models
model.comparison(mod_ses, mod_ses_minority)
```

-   2.326 % improvement in residual fit

-   18.48% improvement in intercept variance

-   SES unexplained variance reduced by 22.83 %

```{r}
# see the difference with 3 example schools
compare.fits(MathAch~SES | Minority + School, mod_ses, mod_ses_minority, data = math)
```

### 

## What about a regular linear models

-   some additional ggplots will help with understanding variables

```{r}
# it's a factor
math$School <- as.factor(math$School)
```

### 1 Predictor

```{r}
# SES with Minority colouring
ggplot(math, aes(x=SES, y=MathAch, colour=Minority)) + 
  geom_point(size=0.1) + 
  geom_smooth(method = "lm", formula = 'y ~ x', se = TRUE, color="blue")

# model 1 factor 
lm1 <- lm(MathAch ~ SES, data = math)
summary(lm1)
visualize(lm1, plot="model")
visualize(lm1, plot="residuals")

```

#### Comments

-   It's not obvious from the scatter-plot points that SES will be a good predictor - though regression line with SE shows the correlation.

-   MinorityYes points can be seen emerging in bottom left - less SES \~ less MathAch

-   SES is highly significant though the Adjusted R-squared is small (0.13)

### 2 Predictors

```{r}
# Belt and braces: Minority box and violin
ggplot(math, aes(x=Minority, y=MathAch, colour=Minority)) + 
  geom_violin() + 
  geom_jitter(shape=16, position=position_jitter(0.4), size=0.3) +
  geom_boxplot(width=0.1, colour="black") 

# and let's have a look at Sex now we are here
ggplot(math, aes(x=Sex, y=MathAch, colour=Sex)) + 
  geom_violin() + 
  geom_jitter(shape=16, position=position_jitter(0.4), size=0.3) +
  geom_boxplot(width=0.1, colour="black") 

# 2 factors
lm2 <- lm(MathAch ~ SES + Minority, data = math)
summary(lm2)
visualize(lm2, plot="model")
visualize(lm2, plot="residuals")

```

#### Comments

-   Box Violin plot show clearly the effect of Minority

-   Median is about 5 points down for Yes

-   SES and Minority together are better predictors

-   How to visualize a model using 2 or more indepandant variables?

    -   look at this nice example plucked from the interweb

```{r}
# read dataset
df = mtcars

# create multiple linear model
lm_fit <- lm(mpg ~ cyl + hp, data=df)
#summary(lm_fit)

# save predictions of the model in the new data frame 
# together with variable you want to plot against
predicted_df <- data.frame(mpg_pred = predict(lm_fit, df), hp=df$hp)

# this is the predicted line of multiple linear regression
ggplot(data = df, aes(x = mpg, y = hp)) + 
  geom_point(color='blue') +
  geom_line(color='red',data = predicted_df, aes(x=mpg_pred, y=hp))
```

-   And now the same approach for the 2 factor model, lm2:

    -   lm2 \<- lm(MathAch \~ SES + Minority, data = math)

```{r}
# save predictions of the model in the new data frame 
# together with variable you want to plot against
predicted_df <- data.frame(MathArc_pred = predict(lm2, math), SES=math$SES)

# this is the predicted line of multiple linear regression
ggplot(data = math, aes(x = SES, y = MathAch)) + 
  geom_point(color='blue',size=0.1) +
  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))
```

### 3 Predictors

```{r}
# interesting 
# what does this mean.. 3 predictors...shows
lm3 <- lm(MathAch ~ SES + Minority + School, data = math)
# summary(lm3)
# undo above comment to go down the hole
#
# Does this mean that the model with SES + Minority is better for some schools?
# TO DO
```

```{r}
# 3 predictors
lm3 <- lm(MathAch ~ SES + Minority + MEANSES, data = math)
summary(lm3)
predicted_df <- data.frame(MathArc_pred = predict(lm3, math), SES=math$SES)
# this is the predicted line of multiple linear regression
ggplot(data = math, aes(x = SES, y = MathAch)) + 
  geom_point(color='blue',size=0.1) +
  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))

ggplot(data = math, aes(x = SES, y = MathAch)) + 
  geom_point(color='blue',size=0.1) +
  geom_point(color='red',size=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))
```

### 4 Predictors

```{r}
# 4 predictors
lm4 <- lm(MathAch ~ SES + Minority + MEANSES + Sex, data = math)
summary(lm4)
predicted_df <- data.frame(MathArc_pred = predict(lm4, math), SES=math$SES)
# this is the predicted line of multiple linear regression
ggplot(data = math, aes(x = SES, y = MathAch)) + 
  geom_point(color='blue',size=0.1) +
  geom_line(color='red',linewidth=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))

ggplot(data = math, aes(x = SES, y = MathAch)) + 
  geom_point(color='blue',size=0.1) +
  geom_point(color='red',size=0.1, data = predicted_df, aes(x=SES, y=MathArc_pred))
```

## Compare models

```{r}
# compare standard 1 and 2 predictor models
model.comparison(lm1, lm2)
```

#### Comments

-   lm2 better than lm1

```{r}
# compare 2 and 3 predictor models
model.comparison(lm2, lm3)
```

#### Comments

-   lm3 better than lm2

```{r}
# compare 3 and 4 predictor models
model.comparison(lm3, lm4)
```

#### Comments

-   lm4 better than lm3

```{r}
# compare 1 and 4 predictor models
model.comparison(lm1, lm4)
```

#### Comments

-   bayes.factor seems to be additive lm1 \<- lm2 \<- lm3 \<-lm4
-   lm4 is much better than lm1

```{r}
# compare the standard 2 predictor model with the MM with 2 predictors 
model.comparison(lm2, mod_ses_minority)
```

#### Comments

-   MM better than standard 2 predictor variable model?
-   
-   

```{r}
# compare the standard 2 predictor model with the MM with 2 predictors 
model.comparison(lm4, mod_ses_minority)
```

#### Comments

-   ..
